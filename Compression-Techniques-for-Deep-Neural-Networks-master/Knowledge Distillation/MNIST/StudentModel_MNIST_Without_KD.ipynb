{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT-WQCRF0Amn"
   },
   "outputs": [],
   "source": [
    "# Author: Naveen Lalwani\n",
    "# Script to train Student model on MNIST without distilling any knowledge\n\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import lite\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLFordX_0Amt"
   },
   "source": [
    "Importing the <B>MNIST</B> dataset from the Tensorflow examples. The images get saved in the folder named \"MNIST_data\" until otherwise specifed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "WcxMqGMU0Amu",
    "outputId": "833849d2-181d-4740-af19-8311bd31b208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "testData, testLabels = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZaBef3W0Amx"
   },
   "source": [
    "The total number of training examples in MNIST dataset is 55,000 has a total of 10 labels. Loading the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0a6Zr0Sn0Amy"
   },
   "outputs": [],
   "source": [
    "numTrain = mnist.train.images.shape[0]\n",
    "trainingData = mnist.train.images[:numTrain,:]\n",
    "trainingLabels = mnist.train.labels[:numTrain,:]\n",
    "\n",
    "numTest = mnist.test.images.shape[0]\n",
    "testData = mnist.test.images[:numTest,:]\n",
    "testLabels = mnist.test.labels[:numTest,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgYGe0j80Am0"
   },
   "source": [
    "<B>Building of Single hidden layer with 100 units neural network. </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Trih-FA70Am1"
   },
   "source": [
    "##Defining the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MYls67S0Am2"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 300\n",
    "batch_size = 256\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JS_99rf0Am5"
   },
   "source": [
    "##Network Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyy1YFa10Am6"
   },
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trxYR_490Am9"
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_input], name = \"X\") # Placeholder for Images\n",
    "Y = tf.placeholder(tf.float32, shape = [None, n_classes], name = \"Y\") # Placeholder for Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49gkPQgA0Am_"
   },
   "source": [
    "##Defining variables to be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIVHzDaH0AnA"
   },
   "source": [
    "##Defining weights & biases for the three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylSiK0iZ0AnB"
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # Fully Connected Layer 1: 784 input channels, 100 output channels\n",
    "    'w1' : tf.Variable(tf.random_normal([784, 50]), name = 'w1'),\n",
    "    # Fully Connected Layer 2: 100 input channels, 10 (number of classes) output channels\n",
    "    'w2' : tf.Variable(tf.random_normal([50, 10]), name = 'w2')\n",
    "}\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([50]), name = 'b1'),\n",
    "    'b2' : tf.Variable(tf.random_normal([10]), name = 'b2')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXW12z6L0AnD"
   },
   "source": [
    "##Defining the model with single hidden layer having 50 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsP9955a0AnD"
   },
   "outputs": [],
   "source": [
    "def model_50(x, weight, bias):\n",
    "    # Fully Connected Layer 1\n",
    "    fc1 = tf.add(tf.matmul(x, weight['w1']), bias['b1']) # Linear Function\n",
    "    fc1 = tf.nn.relu(fc1) # Activation Function\n",
    "    \n",
    "    # Ouput Layer\n",
    "    out = tf.add(tf.matmul(fc1, weight['w2']), bias['b2']) # Output Layer\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JzEDvuvf0AnF"
   },
   "source": [
    "##Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwgh19ZI0AnG"
   },
   "outputs": [],
   "source": [
    "# Get probabilities for the input for all the classes\n",
    "logits = model_50(X, weights, biases)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Since we have the cost in 'loss_op', variable, we need an optimizer to reduce the cost.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "# Minimize the optimization function i.e. minimize the loss.\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Get correct prediction by getting class with maximum probability and get accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "\n",
    "# This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True \n",
    "# becomes 1, and then calculating the average of these numbers.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCGrYfrh0AnI"
   },
   "source": [
    "#Initializing all the variables & Running tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDyBVrLN0AnJ"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1267
    },
    "colab_type": "code",
    "id": "0ptD5vxg0AnM",
    "outputId": "20900f80-7dea-4fa9-e327-33725884e033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Cost: None, Accuracy on batch: 27.34375 %\n",
      "Train Accuracy after 10 on training data:  29.730910062789917 %\n",
      "Epoch 20, Cost: None, Accuracy on batch: 49.21875 %\n",
      "Train Accuracy after 20 on training data:  53.812724351882935 %\n",
      "Epoch 30, Cost: None, Accuracy on batch: 63.28125 %\n",
      "Train Accuracy after 30 on training data:  64.68363404273987 %\n",
      "Epoch 40, Cost: None, Accuracy on batch: 68.75 %\n",
      "Train Accuracy after 40 on training data:  70.30909061431885 %\n",
      "Epoch 50, Cost: None, Accuracy on batch: 73.4375 %\n",
      "Train Accuracy after 50 on training data:  73.14363718032837 %\n",
      "Epoch 60, Cost: None, Accuracy on batch: 75.390625 %\n",
      "Train Accuracy after 60 on training data:  75.28545260429382 %\n",
      "Epoch 70, Cost: None, Accuracy on batch: 76.5625 %\n",
      "Train Accuracy after 70 on training data:  76.91272497177124 %\n",
      "Epoch 80, Cost: None, Accuracy on batch: 74.609375 %\n",
      "Train Accuracy after 80 on training data:  78.86727452278137 %\n",
      "Epoch 90, Cost: None, Accuracy on batch: 77.34375 %\n",
      "Train Accuracy after 90 on training data:  79.72727417945862 %\n",
      "Epoch 100, Cost: None, Accuracy on batch: 83.203125 %\n",
      "Train Accuracy after 100 on training data:  80.70726990699768 %\n",
      "Epoch 110, Cost: None, Accuracy on batch: 86.328125 %\n",
      "Train Accuracy after 110 on training data:  81.52727484703064 %\n",
      "Epoch 120, Cost: None, Accuracy on batch: 82.8125 %\n",
      "Train Accuracy after 120 on training data:  82.30363726615906 %\n",
      "Epoch 130, Cost: None, Accuracy on batch: 82.421875 %\n",
      "Train Accuracy after 130 on training data:  83.04363489151001 %\n",
      "Epoch 140, Cost: None, Accuracy on batch: 84.375 %\n",
      "Train Accuracy after 140 on training data:  83.21272730827332 %\n",
      "Epoch 150, Cost: None, Accuracy on batch: 86.71875 %\n",
      "Train Accuracy after 150 on training data:  84.03090834617615 %\n",
      "Epoch 160, Cost: None, Accuracy on batch: 83.59375 %\n",
      "Train Accuracy after 160 on training data:  84.3327283859253 %\n",
      "Epoch 170, Cost: None, Accuracy on batch: 83.59375 %\n",
      "Train Accuracy after 170 on training data:  84.54363346099854 %\n",
      "Epoch 180, Cost: None, Accuracy on batch: 82.03125 %\n",
      "Train Accuracy after 180 on training data:  84.72727537155151 %\n",
      "Epoch 190, Cost: None, Accuracy on batch: 82.421875 %\n",
      "Train Accuracy after 190 on training data:  85.37999987602234 %\n",
      "Epoch 200, Cost: None, Accuracy on batch: 88.28125 %\n",
      "Train Accuracy after 200 on training data:  85.52363514900208 %\n",
      "Epoch 210, Cost: None, Accuracy on batch: 83.984375 %\n",
      "Train Accuracy after 210 on training data:  86.0909104347229 %\n",
      "Epoch 220, Cost: None, Accuracy on batch: 90.625 %\n",
      "Train Accuracy after 220 on training data:  85.81818342208862 %\n",
      "Epoch 230, Cost: None, Accuracy on batch: 87.890625 %\n",
      "Train Accuracy after 230 on training data:  86.23272776603699 %\n",
      "Epoch 240, Cost: None, Accuracy on batch: 84.375 %\n",
      "Train Accuracy after 240 on training data:  86.42727136611938 %\n",
      "Epoch 250, Cost: None, Accuracy on batch: 89.453125 %\n",
      "Train Accuracy after 250 on training data:  87.23999857902527 %\n",
      "Epoch 260, Cost: None, Accuracy on batch: 90.234375 %\n",
      "Train Accuracy after 260 on training data:  86.9454562664032 %\n",
      "Epoch 270, Cost: None, Accuracy on batch: 89.453125 %\n",
      "Train Accuracy after 270 on training data:  87.3236358165741 %\n",
      "Epoch 280, Cost: None, Accuracy on batch: 83.984375 %\n",
      "Train Accuracy after 280 on training data:  87.58181929588318 %\n",
      "Epoch 290, Cost: None, Accuracy on batch: 89.84375 %\n",
      "Train Accuracy after 290 on training data:  87.23090887069702 %\n",
      "Epoch 300, Cost: None, Accuracy on batch: 83.984375 %\n",
      "Train Accuracy after 300 on training data:  87.53091096878052 %\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Optimization Finished\n",
      "\n",
      "Now testing accuracy on the complete data, we have:\n",
      "\n",
      "Training Accuracy: 0.8753091\n",
      "Test Accuracy: 0.8657\n",
      "Inference Time:  0.044281721115112305\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "INFO:tensorflow:Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[: batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Running Initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    # For saving cost history and accuracy history on batches\n",
    "    cost_hist, acc_hist = [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batch_x, batch_y = next_batch(batch_size, trainingData, trainingLabels)\n",
    "        sess.run(train_op, feed_dict = { X : batch_x, Y : batch_y })\n",
    "        if epoch % display_step == 0:\n",
    "            # Calculating Loss and Accuracy on the current Epoch\n",
    "            loss, acc = sess.run([train_op, accuracy], feed_dict = { X : batch_x, Y : batch_y })\n",
    "            cost_hist.append(loss)\n",
    "            acc_hist.append(acc)\n",
    "            print('Epoch ' + str(epoch) + ', Cost: ' + str(loss) + ', Accuracy on batch: ' + str(acc * 100) + ' %')\n",
    "            print(\"Train Accuracy after \" + str(epoch) + \" on training data: \", str(accuracy.eval({X:trainingData, Y:trainingLabels}) * 100) + ' %')\n",
    "    \n",
    "    print('-' * 70)\n",
    "    print('\\nOptimization Finished\\n')\n",
    "    print('Now testing accuracy on the complete data, we have:\\n')\n",
    "    print(\"Training Accuracy:\", accuracy.eval({X:trainingData, Y:trainingLabels}))\n",
    "    start = time.time()\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X:testData, Y:testLabels}))\n",
    "    end = time.time()\n",
    "    print(\"Inference Time: \", (end-start))\n",
    "    \n",
    "    # Saving the full precision model\n",
    "    converter = lite.TFLiteConverter.from_session(sess, [X], [logits])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"Student_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Q5egYyH0AnP"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "StudentModel.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
