{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Naveen Lalwani\n",
    "# Script to train and Quantize baseline model LeNet-5 on CIFAR-10 dataset and save it\n\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import lite\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 examples in a mini-batch, smaller batch size means more updates in one epoch\n",
    "batch_size = 64 \n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "learning_rate = 0.00025\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading CIFAR-10 Dataset and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) =  tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Enabling One Hot Encoding\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Changing input image datatype to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizaig data\n",
    "x_train  /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3], name = \"X\") # Placeholder for Images 32 X 32 size with 3 RGB channels\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 10], name = \"Y\") # Placeholder for Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet(x):    \n",
    "    '''\n",
    "    LeNet-5 is a 5 layer neural network that takes in input of size 32x32\n",
    "        \n",
    "    The first layer is a convolution layer that has 6 filters with kernel size 5x5\n",
    "    After convolution, the output will go through ReLU activation function.\n",
    "    After ReLU, the output activaations are down-sampled by a max pooling layer with kernel size 2x2\n",
    "\n",
    "    The second layer is a convolution layer that has 16 filters with kernel size 5x5\n",
    "    After convolution, the output will go through ReLU activation function.\n",
    "    After ReLU, the output activaations are down-sampled by a max pooling layer with kernel size 2x2\n",
    "\n",
    "    The third layer is a fully-connected layer with 120 hidden units.\n",
    "    After fully-connected, the output will go through ReLU activation function.\n",
    "\n",
    "    The fourth layer is a fully-connected layer with 84 hidden units.\n",
    "    After fully-connected, the output will go through ReLU activation function.\n",
    "\n",
    "    The last layer is a fully-connected layer that ouputs 10 units (10 classes for MNIST)\n",
    "   \n",
    "   '''\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1    \n",
    "    \n",
    "    weights = {\n",
    "        # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "        'conv1': tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma)),\n",
    "        'conv2': tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)),\n",
    "        'fl1': tf.Variable(tf.truncated_normal(shape=(5 * 5 * 16, 120), mean = mu, stddev = sigma)),\n",
    "        'fl2': tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma)),\n",
    "        'out': tf.Variable(tf.truncated_normal(shape=(84, num_classes), mean = mu, stddev = sigma))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        # The shape of the filter bias is (output_depth,)\n",
    "        'conv1': tf.Variable(tf.zeros(6)),\n",
    "        'conv2': tf.Variable(tf.zeros(16)),\n",
    "        'fl1': tf.Variable(tf.zeros(120)),\n",
    "        'fl2': tf.Variable(tf.zeros(84)),\n",
    "        'out': tf.Variable(tf.zeros(num_classes))\n",
    "    }\n",
    "    \n",
    "    b_min1 = tf.reduce_min(biases['conv1'])\n",
    "    b_max1 = tf.reduce_max(biases['conv1'])\n",
    "    b_fake_quant1 = tf.fake_quant_with_min_max_vars(biases['conv1'], \n",
    "                    min=b_min1, \n",
    "                    max=b_max1, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b1\")\n",
    "    \n",
    "    b_min2 = tf.reduce_min(biases['conv2'])\n",
    "    b_max2 = tf.reduce_max(biases['conv2'])\n",
    "    b_fake_quant2 = tf.fake_quant_with_min_max_vars(biases['conv2'], \n",
    "                    min=b_min2, \n",
    "                    max=b_max2, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b2\")\n",
    "    \n",
    "    b_min3 = tf.reduce_min(biases['fl1'])\n",
    "    b_max3 = tf.reduce_max(biases['fl1'])\n",
    "    b_fake_quant3 = tf.fake_quant_with_min_max_vars(biases['fl1'], \n",
    "                    min=b_min3, \n",
    "                    max=b_max3, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b3\")\n",
    "    \n",
    "    b_min4 = tf.reduce_min(biases['fl2'])\n",
    "    b_max4 = tf.reduce_max(biases['fl2'])\n",
    "    b_fake_quant4 = tf.fake_quant_with_min_max_vars(biases['fl2'], \n",
    "                    min=b_min4, \n",
    "                    max=b_max4, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b4\")\n",
    "    \n",
    "    b_min5 = tf.reduce_min(biases['out'])\n",
    "    b_max5 = tf.reduce_max(biases['out'])\n",
    "    b_fake_quant5 = tf.fake_quant_with_min_max_vars(biases['out'], \n",
    "                    min=b_min5, \n",
    "                    max=b_max5, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b5\")\n",
    "    \n",
    "    w_min1 = tf.reduce_min(weights['conv1'])\n",
    "    w_max1 = tf.reduce_max(weights['conv1'])\n",
    "    w_fake_quant1 = tf.fake_quant_with_min_max_vars(weights['conv1'], \n",
    "                    min=w_min1, \n",
    "                    max=w_max1, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w1\")\n",
    "    \n",
    "    w_min2 = tf.reduce_min(weights['conv2'])\n",
    "    w_max2 = tf.reduce_max(weights['conv2'])\n",
    "    w_fake_quant2 = tf.fake_quant_with_min_max_vars(weights['conv2'], \n",
    "                    min=w_min2, \n",
    "                    max=w_max2, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w2\")\n",
    "    \n",
    "    w_min3 = tf.reduce_min(weights['fl1'])\n",
    "    w_max3 = tf.reduce_max(weights['fl1'])\n",
    "    w_fake_quant2 = tf.fake_quant_with_min_max_vars(weights['fl1'], \n",
    "                    min=w_min3, \n",
    "                    max=w_max3, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w3\")\n",
    "    \n",
    "    w_min4 = tf.reduce_min(weights['fl2'])\n",
    "    w_max4 = tf.reduce_max(weights['fl2'])\n",
    "    w_fake_quant4 = tf.fake_quant_with_min_max_vars(weights['fl2'], \n",
    "                    min=w_min4, \n",
    "                    max=w_max4, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w4\")\n",
    "    \n",
    "    w_min5 = tf.reduce_min(weights['out'])\n",
    "    w_max5 = tf.reduce_max(weights['out'])\n",
    "    w_fake_quant5 = tf.fake_quant_with_min_max_vars(weights['out'], \n",
    "                    min=w_min5, \n",
    "                    max=w_max5, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w5\")\n",
    "\n",
    "\n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['conv1'])\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['conv2'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['conv2'])\n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fl0 = tf.contrib.layers.flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fl1 = tf.add(tf.matmul(fl0, weights['fl1']), biases['fl1'])\n",
    "    # Activation.\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fl2 = tf.add(tf.matmul(fl1, weights['fl2']), biases['fl2'])\n",
    "    # Activation.\n",
    "    fl2 = tf.nn.relu(fl2)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    logits = tf.add(tf.matmul(fl2, weights['out']), biases['out'])\n",
    "                 \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = LeNet(X)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Using ADAM optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Get correct prediction by getting class with maximum probability and get accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "\n",
    "# This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True \n",
    "# becomes 1, and then calculating the average of these numbers.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training------------------\n",
      "Epoch 10, Cost: None, Accuracy on batch: 62.5 %\n",
      "Test Accuracy:  52.95000076293945 %\n",
      "Epoch 20, Cost: None, Accuracy on batch: 62.5 %\n",
      "Test Accuracy:  56.36000037193298 %\n",
      "Epoch 30, Cost: None, Accuracy on batch: 50.0 %\n",
      "Test Accuracy:  59.92000102996826 %\n",
      "Epoch 40, Cost: None, Accuracy on batch: 56.25 %\n",
      "Test Accuracy:  61.37999892234802 %\n",
      "Epoch 50, Cost: None, Accuracy on batch: 62.5 %\n",
      "Test Accuracy:  61.62999868392944 %\n",
      "Epoch 60, Cost: None, Accuracy on batch: 68.75 %\n",
      "Test Accuracy:  61.86000108718872 %\n",
      "Epoch 70, Cost: None, Accuracy on batch: 68.75 %\n",
      "Test Accuracy:  62.529999017715454 %\n",
      "Epoch 80, Cost: None, Accuracy on batch: 87.5 %\n",
      "Test Accuracy:  62.41999864578247 %\n",
      "Epoch 90, Cost: None, Accuracy on batch: 87.5 %\n",
      "Test Accuracy:  63.099998235702515 %\n",
      "Epoch 100, Cost: None, Accuracy on batch: 87.5 %\n",
      "Test Accuracy:  61.29999756813049 %\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Optimization Finished\n",
      "\n",
      "Now testing accuracy on the complete data, we have:\n",
      "\n",
      "Train Accuracy:  79.51000034809113 %\n",
      "Test Accuracy:  61.29999756813049 %\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# Set to use GPU for training Convolution layers\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    num_examples = len(x_train)\n",
    "    acc_hist = []\n",
    "    cost_hist = []\n",
    "    batch_x = x_train\n",
    "    batch_y = y_train\n",
    "    print(\"Training------------------\")\n",
    "    for i in range(1, epochs + 1):\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = x_train[offset:end], y_train[offset:end]\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y}) \n",
    "        if (i % display_step == 0):\n",
    "            loss, acc = sess.run([train_op, accuracy], feed_dict = {X: batch_x, Y: batch_y})\n",
    "            loss = loss\n",
    "            cost_hist.append(loss)\n",
    "            acc_hist.append(acc)\n",
    "            print('Epoch ' + str(i) + ', Cost: ' + str(loss) + ', Accuracy on batch: ' + str(acc * 100) + ' %')\n",
    "            print(\"Test Accuracy: \", str(accuracy.eval({ X : x_test, Y : y_test}) * 100) + ' %')\n",
    "    print('-' * 70)\n",
    "    print('\\nOptimization Finished\\n')\n",
    "    print('Now testing accuracy on the complete data, we have:\\n')\n",
    "    acc1 = 0\n",
    "    for i in range(0, num_examples, 5000):\n",
    "        end = i + 5000\n",
    "        acc1 = acc1 + accuracy.eval({ X : x_train[i:end], Y : y_train[i:end]})\n",
    "    print(\"Train Accuracy: \", str((acc1 / 10) * 100) + ' %')\n",
    "    print(\"Test Accuracy: \", str(accuracy.eval({ X : x_test, Y : y_test}) * 100) + ' %')\n",
    "    \n",
    "    # Saving the full precision model\n",
    "    converter = lite.TFLiteConverter.from_session(sess, [X], [logits])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"LeNet5_float32_model_CIFAR-10_2.tflite\", \"wb\").write(tflite_model)\n",
    "    \n",
    "    # Saving the quantized model\n",
    "    converter = lite.TFLiteConverter.from_session(sess, [X], [logits])\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.default_ranges_stats = (0., 6.)\n",
    "    input_mean = 128\n",
    "    input_stddev = 255\n",
    "    input_arrays = converter.get_input_arrays()\n",
    "    converter.quantized_input_stats = {input_arrays[0] : (input_mean, input_stddev)}\n",
    "    converter.post_training_quantize = True\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"Lenet5_int8_model_CIFAR-10_2.tflite\", \"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
