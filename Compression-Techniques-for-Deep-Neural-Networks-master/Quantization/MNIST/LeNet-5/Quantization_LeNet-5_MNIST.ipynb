{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Naveen Lalwani\n",
    "# Script to train and Quantize baseline model LeNet-5 on MNIST dataset and save it\n\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib import lite\n",
    "from collections import Counter\n\n",
     "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-397eff2f02be>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, one_hot=True)\n",
    "trainingData, trainingLabels = mnist.train.images, mnist.train.labels\n",
    "testData, testLabels = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Since, LeNet architecture accepts 32x32x1 as an input but we have the input shape in the format of 28x28x1. Thus, we use numpy's padding to pad the training and test data with zeros to change its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Padding images with 0s\n",
    "trainingData = np.pad(trainingData, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "testData = np.pad(testData, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_train, X_labels = trainingData, trainingLabels     \n",
    "print(\"Updated Image Shape: {}\".format(trainingData[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, shape = [None, 32, 32, 1], name = \"X\") # Placeholder for Images\n",
    "Y = tf.placeholder(tf.float32, shape = [None, n_classes], name = \"Y\") # Placeholder for Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_5(x):    \n",
    "    '''\n",
    "    LeNet-5 is a 5 layer neural network that takes in input of size 32x32\n",
    "        \n",
    "    The first layer is a convolution layer that has 6 filters with kernel size 5x5\n",
    "    After convolution, the output will go through ReLU activation function.\n",
    "    After ReLU, the output activaations are down-sampled by a max pooling layer with kernel size 2x2\n",
    "\n",
    "    The second layer is a convolution layer that has 16 filters with kernel size 5x5\n",
    "    After convolution, the output will go through ReLU activation function.\n",
    "    After ReLU, the output activaations are down-sampled by a max pooling layer with kernel size 2x2\n",
    "\n",
    "    The third layer is a fully-connected layer with 120 hidden units.\n",
    "    After fully-connected, the output will go through ReLU activation function.\n",
    "\n",
    "    The fourth layer is a fully-connected layer with 84 hidden units.\n",
    "    After fully-connected, the output will go through ReLU activation function.\n",
    "\n",
    "    The last layer is a fully-connected layer that ouputs 10 units (10 classes for MNIST)\n",
    "   \n",
    "   '''\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1    \n",
    "    \n",
    "    weights = {\n",
    "        # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "        'conv1': tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma)),\n",
    "        'conv2': tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)),\n",
    "        'fl1': tf.Variable(tf.truncated_normal(shape=(5 * 5 * 16, 120), mean = mu, stddev = sigma)),\n",
    "        'fl2': tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma)),\n",
    "        'out': tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        # The shape of the filter bias is (output_depth,)\n",
    "        'conv1': tf.Variable(tf.zeros(6)),\n",
    "        'conv2': tf.Variable(tf.zeros(16)),\n",
    "        'fl1': tf.Variable(tf.zeros(120)),\n",
    "        'fl2': tf.Variable(tf.zeros(84)),\n",
    "        'out': tf.Variable(tf.zeros(n_classes))\n",
    "    }\n",
    "    \n",
    "    b_min1 = tf.reduce_min(biases['conv1'])\n",
    "    b_max1 = tf.reduce_max(biases['conv1'])\n",
    "    b_fake_quant1 = tf.fake_quant_with_min_max_vars(biases['conv1'], \n",
    "                    min=b_min1, \n",
    "                    max=b_max1, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b1\")\n",
    "    \n",
    "    b_min2 = tf.reduce_min(biases['conv2'])\n",
    "    b_max2 = tf.reduce_max(biases['conv2'])\n",
    "    b_fake_quant2 = tf.fake_quant_with_min_max_vars(biases['conv2'], \n",
    "                    min=b_min2, \n",
    "                    max=b_max2, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b2\")\n",
    "    \n",
    "    b_min3 = tf.reduce_min(biases['fl1'])\n",
    "    b_max3 = tf.reduce_max(biases['fl1'])\n",
    "    b_fake_quant3 = tf.fake_quant_with_min_max_vars(biases['fl1'], \n",
    "                    min=b_min3, \n",
    "                    max=b_max3, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b3\")\n",
    "    \n",
    "    b_min4 = tf.reduce_min(biases['fl2'])\n",
    "    b_max4 = tf.reduce_max(biases['fl2'])\n",
    "    b_fake_quant4 = tf.fake_quant_with_min_max_vars(biases['fl2'], \n",
    "                    min=b_min4, \n",
    "                    max=b_max4, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b4\")\n",
    "    \n",
    "    b_min5 = tf.reduce_min(biases['out'])\n",
    "    b_max5 = tf.reduce_max(biases['out'])\n",
    "    b_fake_quant5 = tf.fake_quant_with_min_max_vars(biases['out'], \n",
    "                    min=b_min5, \n",
    "                    max=b_max5, \n",
    "                    narrow_range=True,\n",
    "                    name=\"b5\")\n",
    "    \n",
    "    w_min1 = tf.reduce_min(weights['conv1'])\n",
    "    w_max1 = tf.reduce_max(weights['conv1'])\n",
    "    w_fake_quant1 = tf.fake_quant_with_min_max_vars(weights['conv1'], \n",
    "                    min=w_min1, \n",
    "                    max=w_max1, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w1\")\n",
    "    \n",
    "    w_min2 = tf.reduce_min(weights['conv2'])\n",
    "    w_max2 = tf.reduce_max(weights['conv2'])\n",
    "    w_fake_quant2 = tf.fake_quant_with_min_max_vars(weights['conv2'], \n",
    "                    min=w_min2, \n",
    "                    max=w_max2, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w2\")\n",
    "    \n",
    "    w_min3 = tf.reduce_min(weights['fl1'])\n",
    "    w_max3 = tf.reduce_max(weights['fl1'])\n",
    "    w_fake_quant2 = tf.fake_quant_with_min_max_vars(weights['fl1'], \n",
    "                    min=w_min3, \n",
    "                    max=w_max3, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w3\")\n",
    "    \n",
    "    w_min4 = tf.reduce_min(weights['fl2'])\n",
    "    w_max4 = tf.reduce_max(weights['fl2'])\n",
    "    w_fake_quant4 = tf.fake_quant_with_min_max_vars(weights['fl2'], \n",
    "                    min=w_min4, \n",
    "                    max=w_max4, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w4\")\n",
    "    \n",
    "    w_min5 = tf.reduce_min(weights['out'])\n",
    "    w_max5 = tf.reduce_max(weights['out'])\n",
    "    w_fake_quant5 = tf.fake_quant_with_min_max_vars(weights['out'], \n",
    "                    min=w_min5, \n",
    "                    max=w_max5, \n",
    "                    narrow_range=True,\n",
    "                    name=\"w5\")\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['conv1'])\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['conv2'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['conv2'])\n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fl0 = tf.contrib.layers.flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fl1 = tf.add(tf.matmul(fl0, weights['fl1']), biases['fl1'])\n",
    "    # Activation.\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fl2 = tf.add(tf.matmul(fl1, weights['fl2']), biases['fl2'])\n",
    "    # Activation.\n",
    "    fl2 = tf.nn.relu(fl2)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    logits = tf.add(tf.matmul(fl2, weights['out']), biases['out'])\n",
    "                 \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    }
   ],
   "source": [
    "logits = LeNet_5(X)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Get correct prediction by getting class with maximum probability and get accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "\n",
    "# This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True \n",
    "# becomes 1, and then calculating the average of these numbers.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Cost: None, Accuracy on batch: 97.72727489471436 %\n",
      "Train Accuracy after 1 on training data:  93.51260364055634 %\n",
      "Epoch 2, Cost: None, Accuracy on batch: 97.72727489471436 %\n",
      "Train Accuracy after 2 on training data:  96.35447561740875 %\n",
      "Epoch 3, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 3 on training data:  97.32358455657959 %\n",
      "Epoch 4, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 4 on training data:  97.87996113300323 %\n",
      "Epoch 5, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 5 on training data:  98.29815030097961 %\n",
      "Epoch 6, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 6 on training data:  98.58724474906921 %\n",
      "Epoch 7, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 7 on training data:  98.79088699817657 %\n",
      "Epoch 8, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 8 on training data:  98.881796002388 %\n",
      "Epoch 9, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 9 on training data:  98.86543452739716 %\n",
      "Epoch 10, Cost: None, Accuracy on batch: 98.86363744735718 %\n",
      "Train Accuracy after 10 on training data:  99.02725517749786 %\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Optimization Finished\n",
      "\n",
      "Now testing accuracy on the complete data, we have:\n",
      "\n",
      "Test Accuracy:  98.43999743461609 %\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From C:\\Users\\navee\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    num_examples = len(trainingData)\n",
    "    acc_hist = []\n",
    "    cost_hist = []\n",
    "    batch_x = trainingData\n",
    "    batch_y = trainingLabels\n",
    "    for i in range(1, epochs + 1):\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = trainingData[offset:end], trainingLabels[offset:end]\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y}) \n",
    "        loss, acc = sess.run([train_op, accuracy], feed_dict = {X: batch_x, Y: batch_y})\n",
    "        loss = loss\n",
    "        cost_hist.append(loss)\n",
    "        acc_hist.append(acc)\n",
    "        print('Epoch ' + str(i) + ', Cost: ' + str(loss) + ', Accuracy on batch: ' + str(acc * 100) + ' %')\n",
    "        total_acc1 = accuracy.eval({ X : X_train[0:27499], Y : X_labels[0:27499]}) / 2\n",
    "        total_acc2 = accuracy.eval({ X : X_train[27500:55000], Y : X_labels[27500:55000]}) / 2              \n",
    "        print(\"Train Accuracy after \" + str(i) + \" on training data: \", str((total_acc1 + total_acc2) * 100) + ' %')\n",
    "    print('-' * 70)\n",
    "    print('\\nOptimization Finished\\n')\n",
    "    print('Now testing accuracy on the complete data, we have:\\n')\n",
    "    print(\"Test Accuracy: \", str(accuracy.eval({ X : testData, Y : testLabels}) * 100) + ' %')\n",
    "    \n",
    "    # Saving the full precision model\n",
    "    converter = lite.TFLiteConverter.from_session(sess, [X], [logits])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"LeNet5_float32_model_rerun.tflite\", \"wb\").write(tflite_model)\n",
    "    \n",
    "    # Saving the quantized model\n",
    "    converter = lite.TFLiteConverter.from_session(sess, [X], [logits])\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.default_ranges_stats = (0., 6.)\n",
    "    input_mean = 128\n",
    "    input_stddev = 255\n",
    "    input_arrays = converter.get_input_arrays()\n",
    "    converter.quantized_input_stats = {input_arrays[0] : (input_mean, input_stddev)}\n",
    "    converter.post_training_quantize = True\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"LeNet5_int8_model_rerun.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
